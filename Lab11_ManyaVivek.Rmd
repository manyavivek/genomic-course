---
title: "Lab11"
author: "Manya Vivek"
date: "11/9/2020"
output: html_document
---


```{r}
library(dada2); packageVersion("dada2")
```

#### CHANGE ME to the directory containing the fastq files after unzipping.
```{r}
path <- "data/MiSeq_SOP"  
list.files(path)
```
##### Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
```{r}
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
```

##### Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
```{r}
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

# Inspect read quality profiles

##### Visualizing the quality profiles of the foward reads

```{r}
plotQualityProfile(fnFs[1:2])
```

##### Visualize the quality control of the reverse reads 

The reverse reads are of significantly worse quality. Trimming as the average quality crash will improve the alogorithm's sensitivity to rare sequence variants. 
```{r}
plotQualityProfile(fnRs[1:2])
```

# Filter and Trim 

place filtered files in filtered/ subdirectory 
```{r}
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```


```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) # The maxEE parameter sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores.
head(out)
```
# Learn the Error Rates

* Parametric error model (err) and every amplicon dataset has a different set of error rates.

* "LearnErrors" <- method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. 

```{r}
errF <- learnErrors(filtFs, multithread = TRUE)
```
```{r}
errR <- learnErrors(filtRs, multithread = TRUE)
```
```{r}
plotErrors(errF, nominalQ=TRUE)
```

```{r}
plotErrors(errR, nominalQ=TRUE)
```

# Sample Inference 

```{r}
dadaFs <- dada(filtFs, err = errF, multithread = TRUE)
```
```{r}
dadaRs <- dada(filtRs, err = errR, multithread = TRUE)
```
##### Inspecting the returned dada-class object:

```{r}
dadaFs[[1]]
```

# Merge Paired reading 

Merge fowards and reverse reads together to obtain the full denoised sequences. 

```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

<<<<<<< HEAD
# Construct sequence table

```{r}
seqtable <- makeSequenceTable(mergers)
dim(seqtable)
```

```{r}
# The sequence table is a matrix with rows corresponding to the samples, and columns corresponding to the sequence variants. 
table(nchar(getSequences(seqtable)))
```

# Remove Chimeras 

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtable, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```
```{r}
sum(seqtab.nochim)/sum(seqtable)
```

# Track reads through the pipeline

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```
# Assign taxonomy
```{r}
taxa <- assignTaxonomy(seqtab.nochim, "data/silva_nr99_v138_train_set.fa.gz", multithread=TRUE)
```

```{r}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

# Evaluate Accuracy

```{r}
unqs.mock <- seqtab.nochim["Mock",]
unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community.\n")
```

```{r}
mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
match.ref <- sum(sapply(names(unqs.mock), function(x) any(grepl(x, mock.ref))))
cat("Of those,", sum(match.ref), "were exact matches to the expected reference sequences.\n")
```

```{r}
library(phyloseq); packageVersion("phyloseq")
```

```{r}
library(Biostrings); packageVersion("Biostrings")
```

```{r}
library(ggplot2); packageVersion("ggplot2")
```

```{r}
theme_set(theme_bw())
```

```{r}
samples.out <- rownames(seqtab.nochim)
subject <- sapply(strsplit(samples.out, "D"), `[`, 1)
gender <- substr(subject,1,1)
subject <- substr(subject,2,999)
day <- as.integer(sapply(strsplit(samples.out, "D"), `[`, 2))
samdf <- data.frame(Subject=subject, Gender=gender, Day=day)
samdf$When <- "Early"
samdf$When[samdf$Day>100] <- "Late"
rownames(samdf) <- samples.out
```

```{r}
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))
ps <- prune_samples(sample_names(ps) != "Mock", ps) # Remove mock sample
```

```{r}
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps
```

```{r}
plot_richness(ps, x="Day", measures=c("Shannon", "Simpson"), color="When")
```

```{r}
# Transform data to proportions as appropriate for Bray-Curtis distances
ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")
```
```{r}
plot_ordination(ps.prop, ord.nmds.bray, color="When", title="Bray NMDS")
```

```{r}
top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
plot_bar(ps.top20, x="Day", fill="Family") + facet_wrap(~When, scales="free_x")
```


